{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of ten bandit objects initialized in the list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [Bandit(random.random()*4-2) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04364432528747145"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandits[0].pullLever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy():\n",
    "    # TODO: Implement the greedy algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bandits = [Bandit(random.random()*4-2) for _ in range(10)]\n",
    "\n",
    "Qvalues = np.zeros(10)\n",
    "N = np.zeros(10)\n",
    "\n",
    "def run_greedy():\n",
    "    # TODO: Implement the greedy algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "    global Qvalues,N\n",
    "    maxvalue = np.max(Qvalues)\n",
    "    indices = np.where(Qvalues == maxvalue)[0]\n",
    "    chosen_index = np.random.choice(indices)\n",
    "\n",
    "    reward = bandits[chosen_index].pullLever()\n",
    "\n",
    "    N[chosen_index] = N[chosen_index] + 1\n",
    "    Qvalues[chosen_index] = Qvalues[chosen_index] + (reward - Qvalues[chosen_index])/N[chosen_index]\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def main():\n",
    "    x = []\n",
    "    y = []\n",
    "    cumulative_reward = 0\n",
    "    for i in range(1000):\n",
    "        rew = run_greedy()\n",
    "        cumulative_reward = cumulative_reward + rew\n",
    "        x.append(i+1)\n",
    "        y.append(cumulative_reward/(i+1))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Average\")\n",
    "    plt.title(\"Plot of greedy algorithm\")\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphs/greedyalgorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epsilon_greedy(epsilon):\n",
    "    # TODO: Implement the epsilon greedy algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases but for various values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bandits = [Bandit(random.random()*4-2) for _ in range(10)]\n",
    "\n",
    "Qvalues = np.zeros(10)\n",
    "N = np.zeros(10)\n",
    "\n",
    "def run_epsilon_greedy(epsilon):\n",
    "    # TODO: Implement the epsilon greedy algorithm here\n",
    "    global Qvalues,N\n",
    "    prob = np.random.random()\n",
    "    chosen_index = 0\n",
    "    if(prob >= epsilon):\n",
    "        maxvalue = np.max(Qvalues)\n",
    "        indices = np.where(Qvalues == maxvalue)[0]\n",
    "        chosen_index = np.random.choice(indices)\n",
    "    else:\n",
    "        chosen_index = np.random.choice(range(10))\n",
    "\n",
    "    reward = bandits[chosen_index].pullLever()\n",
    "\n",
    "    N[chosen_index] = N[chosen_index] + 1\n",
    "    Qvalues[chosen_index] = Qvalues[chosen_index] + (reward - Qvalues[chosen_index])/N[chosen_index]\n",
    "    \n",
    "    # Return the reward from the bandits in a list\n",
    "    return reward\n",
    "\n",
    "def main():\n",
    "    x = []\n",
    "    y1 = []\n",
    "    cumulativereward = 0\n",
    "    for i in range(1000):\n",
    "        rew = run_epsilon_greedy(0.01)\n",
    "        cumulativereward = cumulativereward + rew\n",
    "        x.append(i+1)\n",
    "        y1.append(cumulativereward/(i+1))\n",
    "    global Qvalues,N \n",
    "    N = np.zeros(10)\n",
    "    Qvalues = np.zeros(10)\n",
    "    cumulativereward = 0\n",
    "    y2 = []\n",
    "    for i in range(1000):\n",
    "        rew = run_epsilon_greedy(0.1)\n",
    "        cumulativereward = cumulativereward + rew\n",
    "        y2.append(cumulativereward/(i+1))\n",
    "    plt.plot(x, y1, label = \"ε = 0.01\", color = \"blue\")\n",
    "    plt.plot(x, y2, label = \"ε = 0.1\", color = \"red\")\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Average\")\n",
    "    plt.title(\"Plot of epsilon greedy algorithm\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphs/epsilongreedyalgorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the $\\epsilon$-greedy algorithm for 1000 iterations and find the optimal $\\epsilon$ value by plotting the cumulative average of rewards for various values of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bandits = [Bandit(random.random()*4-2) for _ in range(10)]\n",
    "\n",
    "Qvalues = np.zeros(10)\n",
    "N = np.zeros(10)\n",
    "\n",
    "def run_epsilon_greedy(epsilon):\n",
    "    # TODO: Implement the epsilon greedy algorithm here\n",
    "    global Qvalues,N\n",
    "    prob = np.random.random()\n",
    "    chosen_index = 0\n",
    "    if(prob >= epsilon):\n",
    "        maxvalue = np.max(Qvalues)\n",
    "        indices = np.where(Qvalues == maxvalue)[0]\n",
    "        chosen_index = np.random.choice(indices)\n",
    "    else:\n",
    "        chosen_index = np.random.choice(range(10))\n",
    "\n",
    "    reward = bandits[chosen_index].pullLever()\n",
    "\n",
    "    N[chosen_index] = N[chosen_index] + 1\n",
    "    Qvalues[chosen_index] = Qvalues[chosen_index] + (reward - Qvalues[chosen_index])/N[chosen_index]\n",
    "    \n",
    "    # Return the reward from the bandits in a list\n",
    "    return reward\n",
    "\n",
    "def main():\n",
    "    x = []\n",
    "    y1 = []\n",
    "    cumulativereward = 0\n",
    "    for i in range(1000):\n",
    "        rew = run_epsilon_greedy(0.01)\n",
    "        cumulativereward = cumulativereward + rew\n",
    "        x.append(i+1)\n",
    "        y1.append(cumulativereward/(i+1))\n",
    "    global Qvalues,N \n",
    "    N = np.zeros(10)\n",
    "    Qvalues = np.zeros(10)\n",
    "    cumulativereward = 0\n",
    "    y2 = []\n",
    "    for i in range(1000):\n",
    "        rew = run_epsilon_greedy(0.1)\n",
    "        cumulativereward = cumulativereward + rew\n",
    "        y2.append(cumulativereward/(i+1))\n",
    "    plt.plot(x, y1, label = \"ε = 0.01\", color = \"blue\")\n",
    "    plt.plot(x, y2, label = \"ε = 0.1\", color = \"red\")\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Average\")\n",
    "    plt.title(\"Plot of epsilon greedy algorithm\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphs/epsilongreedyalgorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimistic_greedy():\n",
    "    # TODO: Implement the optimistic greedy algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cumulative average of rewards as the number of iterations increases for an optimistic greedy of $Q_1 = 10$ and a non-optimistic $\\epsilon = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bandits = [Bandit(random.random()*4-2) for _ in range(10)]\n",
    "\n",
    "optimisticQvalues = np.array([10]*10)\n",
    "optimisticN = np.zeros(10)\n",
    "\n",
    "def run_optimistic_greedy():\n",
    "    # TODO: Implement the optimistic greedy algorithm here\n",
    "    global optimisticQvalues,optimisticN\n",
    "    maxvalue = np.max(optimisticQvalues)\n",
    "    indices = np.where(optimisticQvalues == maxvalue)[0]\n",
    "    chosen_index = np.random.choice(indices)\n",
    "\n",
    "    reward = bandits[chosen_index].pullLever()\n",
    "\n",
    "    optimisticN[chosen_index] = optimisticN[chosen_index] + 1\n",
    "    optimisticQvalues[chosen_index] = optimisticQvalues[chosen_index] + (reward - optimisticQvalues[chosen_index])/optimisticN[chosen_index]\n",
    "    \n",
    "    return reward\n",
    "    # Return the reward from the bandits in a list\n",
    "\n",
    "Qvalues = np.zeros(10)\n",
    "N = np.zeros(10)\n",
    "\n",
    "def run_epsilon_greedy(epsilon):\n",
    "    # TODO: Implement the epsilon greedy algorithm here\n",
    "    global Qvalues,N\n",
    "    prob = np.random.random()\n",
    "    chosen_index = 0\n",
    "    if(prob >= epsilon):\n",
    "        maxvalue = np.max(Qvalues)\n",
    "        indices = np.where(Qvalues == maxvalue)[0]\n",
    "        chosen_index = np.random.choice(indices)\n",
    "    else:\n",
    "        chosen_index = np.random.choice(range(10))\n",
    "\n",
    "    reward = bandits[chosen_index].pullLever()\n",
    "\n",
    "    N[chosen_index] = N[chosen_index] + 1\n",
    "    Qvalues[chosen_index] = Qvalues[chosen_index] + (reward - Qvalues[chosen_index])/N[chosen_index]\n",
    "    \n",
    "    # Return the reward from the bandits in a list\n",
    "    return reward\n",
    "\n",
    "def main():\n",
    "    x = [0,]\n",
    "    y1 = [0,]\n",
    "    cumulative_reward = 0\n",
    "    for i in range(1000):\n",
    "        rew = run_optimistic_greedy()\n",
    "        cumulative_reward = cumulative_reward + rew\n",
    "        print(i+1)\n",
    "        print(cumulative_reward)\n",
    "        print(cumulative_reward/(i+1))\n",
    "        print()\n",
    "        x.append(i+1)\n",
    "        y1.append(cumulative_reward/(i+1))\n",
    "    y2 = [0,]\n",
    "    cumulative_reward = 0\n",
    "    print()\n",
    "    print()\n",
    "    for i in range(1000):\n",
    "        rew = run_epsilon_greedy(0.1)\n",
    "        cumulative_reward = cumulative_reward + rew\n",
    "        y2.append(cumulative_reward/(i+1))\n",
    "    plt.plot(x, y1, label = \"optimistic\" , color = \"red\")\n",
    "    plt.plot(x, y2, label = \"ε = 0.1\" , color = \"blue\")\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Average\")\n",
    "    plt.title(\"Plot of optimistic greedy algorithm\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphs/optimisticgreedyalgorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Upper Confidence Bound (UCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ucb(c):\n",
    "    # TODO: Implement the UCB algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bandits import Bandit\n",
    "import random\n",
    "# Include your imports here, if any are used.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Qvalues = np.zeros(10)\n",
    "N = np.zeros(10)\n",
    "t = 0\n",
    "\n",
    "def run_ucb(c):\n",
    "    # TODO: Implement the UCB algorithm here\n",
    "    # Return the reward from the bandits in a list\n",
    "    global Qvalues,N,t\n",
    "    epsilon = 0.1\n",
    "    prob = np.random.random()\n",
    "    chosen_index = 0\n",
    "    t = t+1\n",
    "    if(prob >= epsilon):\n",
    "        if not ( (N != 0).all() ):\n",
    "            indices = np.where(N == 0)[0]\n",
    "            chosen_index = np.random.choice(indices)\n",
    "        else:\n",
    "            modarr = Qvalues + c * np.sqrt(np.log(t)/N)\n",
    "            maxvalue = np.max(modarr)\n",
    "            indices = np.where(modarr == maxvalue)[0]\n",
    "            chosen_index = np.random.choice(indices)\n",
    "    else:\n",
    "        chosen_index = np.random.choice(range(10))\n",
    "\n",
    "    reward = bandits[chosen_index].pullLever()\n",
    "\n",
    "    N[chosen_index] = N[chosen_index] + 1\n",
    "    Qvalues[chosen_index] = Qvalues[chosen_index] + (reward - Qvalues[chosen_index])/N[chosen_index]\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def main():\n",
    "    x = []\n",
    "    y = []\n",
    "    cumulative_reward = 0\n",
    "    for i in range(1000):\n",
    "        rew = run_ucb(2)\n",
    "        cumulative_reward = cumulative_reward + rew\n",
    "        x.append(i+1)\n",
    "        y.append(cumulative_reward/(i+1))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Average\")\n",
    "    plt.title(\"Plot using upper confidence boost\")\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphs/upperconfidenceboost.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
